<button id="startWakeBtn">Start Assistant</button>
<button id="stopAllBtn">Stop Everything</button>
<p>Status: <span id="status">Waiting for wake word "lilo"...</span></p>
<p>User said: <span id="userText"></span></p>
<p>Assistant said: <span id="assistantText"></span></p>

<script>
  const startWakeBtn = document.getElementById('startWakeBtn');
  const stopAllBtn = document.getElementById('stopAllBtn');
  const statusEl = document.getElementById('status');
  const userTextEl = document.getElementById('userText');
  const assistantTextEl = document.getElementById('assistantText');

  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  const recognition = new SpeechRecognition();

  recognition.lang = 'en-US';
  recognition.continuous = true;
  recognition.interimResults = true;

  let conversationActive = false;
  let finalTranscript = '';
  let pauseTimeout;
  let conversationHistory = [];

  function startListening() {
    finalTranscript = '';
    recognition.start();
    if (conversationActive) {
      statusEl.textContent = "Conversation started. Say 'bye' to end.";
    } else {
      statusEl.textContent = "Waiting for wake word 'lilo'...";
    }
  }

  function stopListening() {
    recognition.stop();
    clearTimeout(pauseTimeout);
  }

  function resetConversation() {
    conversationActive = false;
    conversationHistory = [];
    finalTranscript = '';
    userTextEl.textContent = '';
    assistantTextEl.textContent = '';
    statusEl.textContent = "Waiting for wake word 'lilo'...";
    startListening();
  }

  recognition.onresult = (event) => {
    let interimTranscript = '';

    for (let i = event.resultIndex; i < event.results.length; i++) {
      const spoken = event.results[i][0].transcript.toLowerCase();

      if (!conversationActive && spoken.includes('lilo')) {
        conversationActive = true;
        finalTranscript = '';
        conversationHistory = [];
        statusEl.textContent = "Wake word detected. Starting conversation...";
        return; // wait for next input
      }

      if (conversationActive && spoken.includes('bye')) {
        stopListening();
        assistantTextEl.textContent = "Goodbye!";
        const utterance = new SpeechSynthesisUtterance("Goodbye!");
        speechSynthesis.speak(utterance);
        resetConversation();
        return;
      }

      if (event.results[i].isFinal) {
        finalTranscript += event.results[i][0].transcript + ' ';
      } else {
        interimTranscript += event.results[i][0].transcript;
      }
    }

    userTextEl.textContent = finalTranscript + interimTranscript;

    clearTimeout(pauseTimeout);
    pauseTimeout = setTimeout(() => {
      if (finalTranscript.trim().length > 0 && conversationActive) {
        recognition.stop();
      }
    }, 3000);
  };

  recognition.onend = async () => {
    if (!conversationActive) {
      startListening(); // stay in wake mode
      return;
    }

    const userMessage = finalTranscript.trim();
    if (userMessage.length === 0) {
      startListening();
      return;
    }

    assistantTextEl.textContent = "Thinking...";

    try {
      // Build message history for conversation
      conversationHistory.push({ role: "user", content: userMessage });

      const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': 'Bearer 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',
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: "meta-llama/llama-4-scout-17b-16e-instruct",
          messages: [
            {
              role: "system",
              content: "You are a friendly and helpful assistant named Lilo. Have a natural conversation and remember what the user says during this session."
            },
            ...conversationHistory
          ]
        }),
      });

      const data = await response.json();
      const reply = data.choices?.[0]?.message?.content || "Sorry, I didnâ€™t catch that.";

      // Add assistant reply to history
      conversationHistory.push({ role: "assistant", content: reply });

      assistantTextEl.textContent = reply;
      const utterance = new SpeechSynthesisUtterance(reply);
      speechSynthesis.speak(utterance);
    } catch (err) {
      assistantTextEl.textContent = "Error: " + err.message;
      console.error(err);
    }

    // Resume listening after response
    finalTranscript = '';
    startListening();
  };

  recognition.onerror = (event) => {
    console.error("Speech error:", event.error);
    statusEl.textContent = "Speech recognition error.";
    startListening();
  };

  // Start and stop buttons
  startWakeBtn.onclick = () => {
    stopListening();
    resetConversation();
  };

  stopAllBtn.onclick = () => {
    stopListening();
    statusEl.textContent = "Assistant stopped.";
    conversationActive = false;
  };
</script>
